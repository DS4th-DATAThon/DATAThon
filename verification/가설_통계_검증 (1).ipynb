{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c566f0-b4f7-4414-bbf3-73e0fefae9f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. ê³ ìœ í•œ ê°’, ê²°ì¸¡ì¹˜ê°€ ë§ì€ ì»¬ëŸ¼ë“¤ì„ ì‚­ì œ í›„ ëª¨ë¸ë§í•˜ë©´ ì„±ëŠ¥ í–¥ìƒí•  ê²ƒì´ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "698d8be1-95ae-4762-8b37-bed52e151e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('~/Aiffel/DATAThon/playground-series-s4e11/train.csv')\n",
    "test = pd.read_csv('~/Aiffel/DATAThon/playground-series-s4e11/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3fb2e109-70ba-442a-96fd-2766d4b2a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['Name', 'City', 'Profession', 'Sleep Duration', 'Age', 'Degree','id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e5cf184f-9105-42bb-ab0f-ab7a4146b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "train.drop(columns=['CGPA'], inplace=True)\n",
    "train['Academic_Pressure_missing'] = train['Academic Pressure'].isnull().astype(int)\n",
    "train['Study_Satisfaction_missing'] = train['Study Satisfaction'].isnull().astype(int)\n",
    "imputer_ap = SimpleImputer(strategy='mean')\n",
    "imputer_ss = SimpleImputer(strategy='mean')\n",
    "train['Academic Pressure'] = imputer_ap.fit_transform(train[['Academic Pressure']])\n",
    "train['Study Satisfaction'] = imputer_ss.fit_transform(train[['Study Satisfaction']])\n",
    "train['Work_Pressure_missing'] = train['Work Pressure'].isnull().astype(int)\n",
    "train['Job_Satisfaction_missing'] = train['Job Satisfaction'].isnull().astype(int)\n",
    "imputer_wp = SimpleImputer(strategy='mean')\n",
    "imputer_js = SimpleImputer(strategy='mean')\n",
    "train['Work Pressure'] = imputer_wp.fit_transform(train[['Work Pressure']])\n",
    "train['Job Satisfaction'] = imputer_js.fit_transform(train[['Job Satisfaction']])\n",
    "train = train[train['Financial Stress'].notnull()].copy()\n",
    "\n",
    "valid_dietary = ['Moderate', 'Unhealthy', 'Healthy']\n",
    "train['Dietary Habits'] = train['Dietary Habits'].where(train['Dietary Habits'].isin(valid_dietary))\n",
    "train['Dietary Habits'] = train['Dietary Habits'].fillna(train['Dietary Habits'].mode()[0])\n",
    "\n",
    "binary_cols = ['Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n",
    "for col in binary_cols:\n",
    "    train[col] = train[col].map({'Yes': 1, 'No': 0})\n",
    "train['Gender'] = train['Gender'].map({'Male': 1, 'Female': 0})\n",
    "train['Working Professional or Student'] = train['Working Professional or Student'].map({'Working Professional': 1, 'Student': 0})\n",
    "\n",
    "# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§\n",
    "scale_cols = ['Work/Study Hours', 'Financial Stress']\n",
    "# scale_cols = ['Age', 'Sleep Duration', 'Work/Study Hours', 'Financial Stress']\n",
    "scaler = StandardScaler()\n",
    "train[scale_cols] = scaler.fit_transform(train[scale_cols])\n",
    "\n",
    "train = pd.get_dummies(train, columns=['Dietary Habits'], drop_first=True) # ë‹¤ì¤‘ê³µì„ ì„± ë°©ì§€ë¥¼ ìœ„í•´ ì²« ë²ˆì§¸ ë²”ì£¼ëŠ” ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1b653f6b-fb55-465a-9866-a3a8ab41d247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:40:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:40:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:40:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:40:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:40:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:40:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:40:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:40:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:40:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:40:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (10íšŒ): [0.913  0.9132 0.9135 0.9134 0.9134 0.9126 0.914  0.9118 0.9131 0.9118]\n",
      "Recall (10íšŒ):    [0.6815 0.6733 0.6793 0.6747 0.679  0.6728 0.6823 0.6711 0.6777 0.6731]\n",
      "\n",
      "í‰ê·  Accuracy: 0.9130\n",
      "í‰ê·  Recall:   0.6765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "accuracy_list = []\n",
    "recall_list = []\n",
    "\n",
    "# 10íšŒ ë°˜ë³µ\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        train.drop(columns=['Depression']),\n",
    "        train['Depression'],\n",
    "        test_size=0.3,\n",
    "        stratify=train['Depression'],\n",
    "        random_state=i  # ë°˜ë³µë§ˆë‹¤ seed ë³€ê²½\n",
    "    )\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "\n",
    "    accuracy_list.append(acc)\n",
    "    recall_list.append(rec)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"Accuracy (10íšŒ):\", np.round(accuracy_list, 4))\n",
    "print(\"Recall (10íšŒ):   \", np.round(recall_list, 4))\n",
    "print(f\"\\ní‰ê·  Accuracy: {np.mean(accuracy_list):.4f}\")\n",
    "print(f\"í‰ê·  Recall:   {np.mean(recall_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e7ec1-b817-482d-aba5-253d38990eba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. í•™ìƒ / ì§ì¥ì¸ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì—¬ ëª¨ë¸ë§í•˜ë©´ ì„±ëŠ¥ í–¥ìƒí•  ê²ƒì´ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b8cdb0d2-cbd9-4eb0-b84a-5c88a716dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('~/Aiffel/DATAThon/playground-series-s4e11/train.csv')\n",
    "test = pd.read_csv('~/Aiffel/DATAThon/playground-series-s4e11/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2942d901-2b03-44c5-80b2-e04119bb6f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hf/5rb3dt8x3mq707hsrnthhx2w0000gn/T/ipykernel_1363/966698980.py:95: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  work_train['Profession'].fillna('Missing', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "train.drop(columns=['id'], inplace=True)\n",
    "train.drop(columns=['Name'], inplace=True)\n",
    "\n",
    "# ì´ìƒì¹˜, ì•Œ ìˆ˜ ì—†ëŠ” ê°’ë“¤ ì œê±°\n",
    "delete_values = [\n",
    "    'Indore', 'Pune', 'Moderate', 'Unhealthy', 'Sleep_Duration',\n",
    "    'Work_Study_Hours', 'No', '45', '49 hours', '55-66 hours', '40-45 hours', \n",
    "    '9-5 hours', '10-6 hours', '9-6 hours', '9-5', '45-48 hours', '35-36 hours'\n",
    "]\n",
    "\n",
    "train = train[~train['Sleep Duration'].isin(delete_values)].copy()\n",
    "\n",
    "def convert_sleep_to_hours(val):\n",
    "    try:\n",
    "        val = str(val).strip().lower()\n",
    "\n",
    "        # íŠ¹ë³„ ì²˜ë¦¬: 'than n hours' â†’ 'less than n hours' ê°„ì£¼\n",
    "        if 'than' in val and 'less' not in val and 'more' not in val:\n",
    "            match = re.search(r'\\d+', val)\n",
    "            if match:\n",
    "                return float(match.group()) - 0.5\n",
    "\n",
    "        # Less than n hours â†’ n - 0.5\n",
    "        if 'less than' in val:\n",
    "            match = re.search(r'\\d+', val)\n",
    "            if match:\n",
    "                return float(match.group()) - 0.5\n",
    "\n",
    "        # More than n hours â†’ n + 0.5\n",
    "        elif 'more than' in val:\n",
    "            match = re.search(r'\\d+', val)\n",
    "            if match:\n",
    "                return float(match.group()) + 0.5\n",
    "\n",
    "        # ì •í™•íˆ n hours â†’ ìˆ«ìë§Œ ì¶”ì¶œ\n",
    "        elif re.match(r'^\\d+\\s*hours$', val):\n",
    "            return float(re.findall(r'\\d+', val)[0])\n",
    "\n",
    "        # n-m hours ë˜ëŠ” nâ€“n â†’ í‰ê· ê°’\n",
    "        elif re.search(r'\\d+\\s*[-â€“~]\\s*\\d+', val):\n",
    "            nums = [int(n) for n in re.findall(r'\\d+', val)]\n",
    "            if len(nums) == 2:\n",
    "                return sum(nums) / 2\n",
    "\n",
    "        # ìˆ«ìë§Œ â†’ ê·¸ëŒ€ë¡œ\n",
    "        elif re.match(r'^\\d+(\\.\\d+)?$', val):\n",
    "            return float(val)\n",
    "\n",
    "        # ë‚˜ë¨¸ì§€ëŠ” ì´ìƒê°’ìœ¼ë¡œ ê°„ì£¼\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "train['Sleep Duration'] = train['Sleep Duration'].apply(convert_sleep_to_hours)\n",
    "\n",
    "work_train = train[train['Working Professional or Student'] == 'Working Professional']\n",
    "stu_train = train[train['Working Professional or Student'] == 'Student']\n",
    "\n",
    "#ì§ì¥ì¸ í…Œì´ë¸” ê²°ì¸¡ì¹˜ ì œê±°\n",
    "work_cols_to_drop = ['Academic Pressure', 'Study Satisfaction', 'CGPA']\n",
    "work_train = work_train.drop(columns=work_cols_to_drop)\n",
    "\n",
    "#í•™ìƒ í…Œì´ë¸” ê²°ì¸¡ì¹˜ ì œê±°\n",
    "stu_cols_to_drop = ['Work Pressure', 'Job Satisfaction', 'Profession']\n",
    "stu_train = stu_train.drop(columns=stu_cols_to_drop)\n",
    "\n",
    "# 1. CGPA ì»¬ëŸ¼ ì œê±°\n",
    "stu_train.drop(columns=['CGPA'], inplace=True)\n",
    "\n",
    "# 2. Academic Pressure ê²°ì¸¡ ì—¬ë¶€ íŒŒìƒ ë³€ìˆ˜\n",
    "stu_train['Academic_Pressure_missing'] = stu_train['Academic Pressure'].isnull().astype(int)\n",
    "\n",
    "# 3. Study Satisfaction ê²°ì¸¡ ì—¬ë¶€ íŒŒìƒ ë³€ìˆ˜\n",
    "stu_train['Study_Satisfaction_missing'] = stu_train['Study Satisfaction'].isnull().astype(int)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')  # ë˜ëŠ” 'median', 'most_frequent'\n",
    "stu_train['Academic Pressure'] = imputer.fit_transform(stu_train[['Academic Pressure']])\n",
    "stu_train['Study Satisfaction'] = imputer.fit_transform(stu_train[['Study Satisfaction']])\n",
    "\n",
    "# 4. Work_Pressure_missing ê²°ì¸¡ ì—¬ë¶€ íŒŒìƒ ë³€ìˆ˜\n",
    "work_train['Work_Pressure_missing'] = work_train['Work Pressure'].isnull().astype(int)\n",
    "# 5. Job_Satisfaction_missing ê²°ì¸¡ ì—¬ë¶€ íŒŒìƒ ë³€ìˆ˜\n",
    "work_train['Job_Satisfaction_missing'] = work_train['Job Satisfaction'].isnull().astype(int)\n",
    "\n",
    "work_train['Work Pressure'] = imputer.fit_transform(work_train[['Work Pressure']])\n",
    "work_train['Job Satisfaction'] = imputer.fit_transform(work_train[['Job Satisfaction']])\n",
    "\n",
    "work_train = work_train[work_train['Financial Stress'].notnull()].copy()\n",
    "stu_train = stu_train[stu_train['Financial Stress'].notnull()].copy()\n",
    "\n",
    "work_train['Profession'].fillna('Missing', inplace=True)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scale_cols = ['Age', 'Sleep Duration', 'Work/Study Hours', 'Financial Stress']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "work_train[scale_cols] = scaler.fit_transform(work_train[scale_cols])\n",
    "stu_train[scale_cols] = scaler.fit_transform(stu_train[scale_cols])\n",
    "\n",
    "degree_group_map = {\n",
    "    # ì˜í•™/ë³´ê±´\n",
    "    'MD': 'Medical',\n",
    "    'MBBS': 'Medical',\n",
    "    'B.Pharm': 'Pharmacy',\n",
    "    'M.Pharm': 'Pharmacy',\n",
    "    'MPharm': 'Pharmacy',\n",
    "    'P.Pharm': 'Pharmacy',\n",
    "    'S.Pharm': 'Pharmacy',\n",
    "    'N.Pharm': 'Pharmacy',\n",
    "\n",
    "    # ê³µí•™/ê¸°ìˆ \n",
    "    'B.Tech': 'Engineering',\n",
    "    'M.Tech': 'Engineering',\n",
    "    'ME': 'Engineering',\n",
    "    'MTech': 'Engineering',\n",
    "    'M_Tech': 'Engineering',\n",
    "    'BE': 'Engineering',\n",
    "    'BCA': 'Engineering',\n",
    "    'MCA': 'Engineering',\n",
    "    'E.Tech': 'Engineering',\n",
    "    'S.Tech': 'Engineering',\n",
    "    'LLTech': 'Engineering',\n",
    "    'LLCom': 'Engineering',\n",
    "\n",
    "    # ì¸ë¬¸/ì‚¬íšŒ/ë¹„ì¦ˆë‹ˆìŠ¤\n",
    "    'BBA': 'Business',\n",
    "    'MBA': 'Business',\n",
    "    'M. Business Analyst': 'Business',\n",
    "    'B.Com': 'Commerce',\n",
    "    'M.Com': 'Commerce',\n",
    "    'P.Com': 'Commerce',\n",
    "    'LLB': 'Law',\n",
    "    'LLM': 'Law',\n",
    "    'LLBA': 'Law',\n",
    "    'LL.Com': 'Law',\n",
    "    'LL B.Ed': 'Education',\n",
    "    'B.Ed': 'Education',\n",
    "    'M.Ed': 'Education',\n",
    "    'L.Ed': 'Education',\n",
    "    'K.Ed': 'Education',\n",
    "    'LLEd': 'Education',\n",
    "    'BEd': 'Education',\n",
    "\n",
    "    # ê³¼í•™\n",
    "    'BSc': 'Science',\n",
    "    'MSc': 'Science',\n",
    "    'B.Sc': 'Science',\n",
    "\n",
    "    # ê¸°íƒ€, ê±´ì¶•, í˜¸í…”ê²½ì˜ ë“±\n",
    "    'BHM': 'Hospitality',\n",
    "    'MHM': 'Hospitality',\n",
    "    'B.Arch': 'Architecture',\n",
    "    'M.Arch': 'Architecture',\n",
    "    'BArch': 'Architecture',\n",
    "    'B.B.Arch': 'Architecture',\n",
    "\n",
    "    # í•™ìœ„ ë° í•™êµ ì¡¸ì—…\n",
    "    'PhD': 'PhD',\n",
    "    'Class 12': 'School',\n",
    "    'Class 11': 'School',\n",
    "}\n",
    "\n",
    "work_train['degree_group'] = work_train['Degree'].apply(lambda x: degree_group_map.get(x, 'Other'))\n",
    "stu_train['degree_group'] = stu_train['Degree'].apply(lambda x: degree_group_map.get(x, 'Other'))\n",
    "\n",
    "work_train=work_train.drop('Degree',axis=1)\n",
    "stu_train=stu_train.drop('Degree',axis=1)\n",
    "\n",
    "# ì •ìƒ ê°’ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” NaNìœ¼ë¡œ\n",
    "valid_dietary = ['Moderate', 'Unhealthy', 'Healthy']\n",
    "work_train['Dietary Habits'] = work_train['Dietary Habits'].where(work_train['Dietary Habits'].isin(valid_dietary))\n",
    "stu_train['Dietary Habits'] = stu_train['Dietary Habits'].where(stu_train['Dietary Habits'].isin(valid_dietary))\n",
    "\n",
    "# ê²°ì¸¡ê°’ì€ ìµœë¹ˆê°’ìœ¼ë¡œ ã…\n",
    "work_train['Dietary Habits'] = work_train['Dietary Habits'].fillna(work_train['Dietary Habits'].mode()[0])\n",
    "stu_train['Dietary Habits'] = stu_train['Dietary Habits'].fillna(stu_train['Dietary Habits'].mode()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "45278446-f0a3-463e-a66d-5483321cfc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Gender, Working Status - Binary\n",
    "list = [work_train, stu_train]\n",
    "\n",
    "for train in list :\n",
    "    train['Gender'] = train['Gender'].map({'Male': 1, 'Female': 0})\n",
    "    train['Working Professional or Student'] = train['Working Professional or Student'].map({'Working Professional': 1, 'Student': 0})\n",
    "\n",
    "# Degree, Dietary Habits - One-hot encoding\n",
    "work_train = pd.get_dummies(work_train, columns=['degree_group', 'Dietary Habits'], drop_first=True)\n",
    "stu_train = pd.get_dummies(stu_train, columns=['degree_group', 'Dietary Habits'], drop_first=True)\n",
    "\n",
    "# Profession, City - Target encoding (ë˜ëŠ” frequency encoding)\n",
    "# ì˜ˆ: Target Mean Encoding (Depression ì»¬ëŸ¼ ì‚¬ìš©)\n",
    "for col in ['Profession', 'City']:\n",
    "    target_mean = work_train.groupby(col)['Depression'].mean()\n",
    "    work_train[col + '_target'] = work_train[col].map(target_mean)\n",
    "\n",
    "# ì´í›„ í•„ìš”ì‹œ ì›ë³¸ ì»¬ëŸ¼ ì œê±°\n",
    "work_train.drop(columns=['Profession', 'City'], inplace=True)\n",
    "\n",
    "## 2 ## \n",
    "for col in ['City']:\n",
    "    target_mean = stu_train.groupby(col)['Depression'].mean()\n",
    "    stu_train[col + '_target'] = stu_train[col].map(target_mean)\n",
    "\n",
    "# ì´í›„ í•„ìš”ì‹œ ì›ë³¸ ì»¬ëŸ¼ ì œê±°\n",
    "stu_train.drop(columns=[ 'City'], inplace=True)\n",
    "\n",
    "binary_cols = ['Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n",
    "for col in binary_cols:\n",
    "    work_train[col] = work_train[col].map({'Yes': 1, 'No': 0})\n",
    "    stu_train[col] = stu_train[col].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "567dcc27-c5b0-4c47-af9f-79f83977f221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:46:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š [ì§ì¥ì¸] Accuracy (10íšŒ): [0.9605 0.9623 0.9618 0.9611 0.9605 0.9607 0.9613 0.9635 0.9615 0.9621]\n",
      "ğŸ“Š [ì§ì¥ì¸] Recall    (10íšŒ): [0.6793 0.6876 0.68   0.6717 0.6779 0.6728 0.6746 0.7017 0.6829 0.688 ]\n",
      "â–¶ í‰ê·  Accuracy: 0.9615\n",
      "â–¶ í‰ê·  Recall:   0.6817\n",
      "\n",
      "ğŸ“Š [í•™ìƒ] Accuracy (10íšŒ): [0.8505 0.8454 0.8511 0.846  0.8439 0.8447 0.8506 0.8462 0.846  0.8533]\n",
      "ğŸ“Š [í•™ìƒ] Recall    (10íšŒ): [0.8918 0.8861 0.8916 0.8808 0.8818 0.8827 0.888  0.8857 0.8818 0.8941]\n",
      "â–¶ í‰ê·  Accuracy: 0.8478\n",
      "â–¶ í‰ê·  Recall:   0.8864\n",
      "\n",
      "ğŸ“Š [ì „ì²´] Accuracy (10íšŒ): [0.9387 0.9391 0.9398 0.9383 0.9374 0.9377 0.9394 0.9402 0.9386 0.9405]\n",
      "ğŸ“Š [ì „ì²´] Recall    (10íšŒ): [0.8151 0.8144 0.8152 0.8053 0.8082 0.8069 0.8109 0.8193 0.81   0.8197]\n",
      "â–¶ í‰ê·  Accuracy: 0.9390\n",
      "â–¶ í‰ê·  Recall:   0.8125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# ğŸ“Œ í‰ê°€ ì§€í‘œ ì €ì¥ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "work_acc_list, work_rec_list = [], []\n",
    "stu_acc_list, stu_rec_list = [], []\n",
    "all_acc_list, all_rec_list = [], []\n",
    "\n",
    "# ğŸ“Œ ê³µí†µ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (random_stateëŠ” ë£¨í”„ ë‚´ì—ì„œ ë„£ìŒ)\n",
    "base_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'use_label_encoder': False,\n",
    "    'eval_metric': 'logloss',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# ğŸ“Œ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  í•¨ìˆ˜ ì •ì˜ (seed ì‚¬ìš©)\n",
    "def get_train_test_dataset(df, seed):\n",
    "    y = df['Depression']\n",
    "    X = df.drop('Depression', axis=1)\n",
    "    return train_test_split(X, y, test_size=0.3, stratify=y, random_state=seed)\n",
    "\n",
    "# ğŸ“Œ 10íšŒ ë°˜ë³µ í•™ìŠµ ë° í‰ê°€\n",
    "for seed in range(10):\n",
    "    # ë°ì´í„° ë¶„í•  (seed ì ìš©)\n",
    "    work_X_train, work_X_test, work_y_train, work_y_test = get_train_test_dataset(work_train, seed)\n",
    "    stu_X_train, stu_X_test, stu_y_train, stu_y_test = get_train_test_dataset(stu_train, seed)\n",
    "\n",
    "    # ì§ì¥ì¸ ëª¨ë¸ (seed ì ìš©)\n",
    "    work_model = XGBClassifier(**base_params, random_state=seed)\n",
    "    work_model.fit(work_X_train, work_y_train)\n",
    "    work_y_pred = work_model.predict(work_X_test)\n",
    "    work_acc_list.append(accuracy_score(work_y_test, work_y_pred))\n",
    "    work_rec_list.append(recall_score(work_y_test, work_y_pred))\n",
    "\n",
    "    # í•™ìƒ ëª¨ë¸ (seed ì ìš©)\n",
    "    stu_model = XGBClassifier(**base_params, random_state=seed)\n",
    "    stu_model.fit(stu_X_train, stu_y_train)\n",
    "    stu_y_pred = stu_model.predict(stu_X_test)\n",
    "    stu_acc_list.append(accuracy_score(stu_y_test, stu_y_pred))\n",
    "    stu_rec_list.append(recall_score(stu_y_test, stu_y_pred))\n",
    "\n",
    "    # ì „ì²´ ë°ì´í„° í†µí•© í‰ê°€\n",
    "    y_true_all = np.concatenate([work_y_test, stu_y_test])\n",
    "    y_pred_all = np.concatenate([work_y_pred, stu_y_pred])\n",
    "    all_acc_list.append(accuracy_score(y_true_all, y_pred_all))\n",
    "    all_rec_list.append(recall_score(y_true_all, y_pred_all))\n",
    "\n",
    "# ğŸ“Œ ê²°ê³¼ ì¶œë ¥ í•¨ìˆ˜\n",
    "def print_results(title, acc_list, rec_list):\n",
    "    print(f\"ğŸ“Š [{title}] Accuracy (10íšŒ):\", np.round(acc_list, 4))\n",
    "    print(f\"ğŸ“Š [{title}] Recall    (10íšŒ):\", np.round(rec_list, 4))\n",
    "    print(f\"â–¶ í‰ê·  Accuracy: {np.mean(acc_list):.4f}\")\n",
    "    print(f\"â–¶ í‰ê·  Recall:   {np.mean(rec_list):.4f}\")\n",
    "    print()\n",
    "\n",
    "# ğŸ“Œ ê²°ê³¼ ì¶œë ¥\n",
    "print_results(\"ì§ì¥ì¸\", work_acc_list, work_rec_list)\n",
    "print_results(\"í•™ìƒ\", stu_acc_list, stu_rec_list)\n",
    "print_results(\"ì „ì²´\", all_acc_list, all_rec_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9327326-b853-4c3a-8195-8047a82a5103",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. ê²°ì¸¡ì¹˜ ì„í“¨íŒ… / íŒŒìƒ ë³€ìˆ˜ ë§Œë“  í›„ ëª¨ë¸ë§í•˜ë©´ ì„±ëŠ¥ í–¥ìƒí•  ê²ƒì´ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f20d9ee0-9805-47b1-802f-ddc47609620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('~/Aiffel/DATAThon/playground-series-s4e11/train.csv')\n",
    "test = pd.read_csv('~/Aiffel/DATAThon/playground-series-s4e11/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6351cf51-a109-4b20-8067-68cbdef10355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ ì‚­ì œ\n",
    "train.drop(columns=['id'], inplace=True)\n",
    "train.drop(columns=['Name'], inplace=True)\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "train.drop(columns=['CGPA'], inplace=True)\n",
    "train['Academic_Pressure_missing'] = train['Academic Pressure'].isnull().astype(int)\n",
    "train['Study_Satisfaction_missing'] = train['Study Satisfaction'].isnull().astype(int)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train['Academic Pressure'] = imputer.fit_transform(train[['Academic Pressure']])\n",
    "train['Study Satisfaction'] = imputer.fit_transform(train[['Study Satisfaction']])\n",
    "train['Work_Pressure_missing'] = train['Work Pressure'].isnull().astype(int)\n",
    "train['Job_Satisfaction_missing'] = train['Job Satisfaction'].isnull().astype(int)\n",
    "train['Work Pressure'] = imputer.fit_transform(train[['Work Pressure']])\n",
    "train['Job Satisfaction'] = imputer.fit_transform(train[['Job Satisfaction']])\n",
    "train = train[train['Financial Stress'].notnull()].copy()\n",
    "\n",
    "# ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬\n",
    "delete_values = [\n",
    "    'Indore', 'Pune', 'Moderate', 'Unhealthy', 'Sleep_Duration',\n",
    "    'Work_Study_Hours', 'No', '45', '49 hours', '55-66 hours', '40-45 hours', \n",
    "    '9-5 hours', '10-6 hours', '9-6 hours', '9-5', '45-48 hours', '35-36 hours'\n",
    "]\n",
    "train = train[~train['Sleep Duration'].isin(delete_values)].copy()\n",
    "def convert_sleep_to_hours(val):\n",
    "    try:\n",
    "        val = str(val).strip().lower()\n",
    "        if 'than' in val and 'less' not in val and 'more' not in val:\n",
    "            match = re.search(r'\\d+', val)\n",
    "            if match:\n",
    "                return float(match.group()) - 0.5\n",
    "        if 'less than' in val:\n",
    "            match = re.search(r'\\d+', val)\n",
    "            if match:\n",
    "                return float(match.group()) - 0.5\n",
    "        elif 'more than' in val:\n",
    "            match = re.search(r'\\d+', val)\n",
    "            if match:\n",
    "                return float(match.group()) + 0.5\n",
    "        elif re.match(r'^\\d+\\s*hours$', val):\n",
    "            return float(re.findall(r'\\d+', val)[0])\n",
    "        elif re.search(r'\\d+\\s*[-â€“~]\\s*\\d+', val):\n",
    "            nums = [int(n) for n in re.findall(r'\\d+', val)]\n",
    "            if len(nums) == 2:\n",
    "                return sum(nums) / 2\n",
    "        elif re.match(r'^\\d+(\\.\\d+)?$', val):\n",
    "            return float(val)\n",
    "        else:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "train['Sleep Duration'] = train['Sleep Duration'].apply(convert_sleep_to_hours)\n",
    "degree_group_map = {\n",
    "    'MD': 'Medical',\n",
    "    'MBBS': 'Medical',\n",
    "    'B.Pharm': 'Pharmacy',\n",
    "    'M.Pharm': 'Pharmacy',\n",
    "    'MPharm': 'Pharmacy',\n",
    "    'P.Pharm': 'Pharmacy',\n",
    "    'S.Pharm': 'Pharmacy',\n",
    "    'N.Pharm': 'Pharmacy',\n",
    "    'B.Tech': 'Engineering',\n",
    "    'M.Tech': 'Engineering',\n",
    "    'ME': 'Engineering',\n",
    "    'MTech': 'Engineering',\n",
    "    'M_Tech': 'Engineering',\n",
    "    'BE': 'Engineering',\n",
    "    'BCA': 'Engineering',\n",
    "    'MCA': 'Engineering',\n",
    "    'E.Tech': 'Engineering',\n",
    "    'S.Tech': 'Engineering',\n",
    "    'LLTech': 'Engineering',\n",
    "    'LLCom': 'Engineering',\n",
    "    'BBA': 'Business',\n",
    "    'MBA': 'Business',\n",
    "    'M. Business Analyst': 'Business',\n",
    "    'B.Com': 'Commerce',\n",
    "    'M.Com': 'Commerce',\n",
    "    'P.Com': 'Commerce',\n",
    "    'LLB': 'Law',\n",
    "    'LLM': 'Law',\n",
    "    'LLBA': 'Law',\n",
    "    'LL.Com': 'Law',\n",
    "    'LL B.Ed': 'Education',\n",
    "    'B.Ed': 'Education',\n",
    "    'M.Ed': 'Education',\n",
    "    'L.Ed': 'Education',\n",
    "    'K.Ed': 'Education',\n",
    "    'LLEd': 'Education',\n",
    "    'BEd': 'Education',\n",
    "    'BSc': 'Science',\n",
    "    'MSc': 'Science',\n",
    "    'B.Sc': 'Science',\n",
    "    'BHM': 'Hospitality',\n",
    "    'MHM': 'Hospitality',\n",
    "    'B.Arch': 'Architecture',\n",
    "    'M.Arch': 'Architecture',\n",
    "    'BArch': 'Architecture',\n",
    "    'B.B.Arch': 'Architecture',\n",
    "    'PhD': 'PhD',\n",
    "    'Class 12': 'School',\n",
    "    'Class 11': 'School',\n",
    "}\n",
    "train['degree_group'] =train['Degree'].apply(lambda x: degree_group_map.get(x, 'Other'))\n",
    "train=train.drop('Degree',axis=1)\n",
    "\n",
    "top_cities = train['City'].value_counts().nlargest(15).index\n",
    "train['City'] = train['City'].where(train['City'].isin(top_cities), other='Other')\n",
    "\n",
    "valid_dietary = ['Moderate', 'Unhealthy', 'Healthy']\n",
    "train['Dietary Habits'] = train['Dietary Habits'].where(train['Dietary Habits'].isin(valid_dietary))\n",
    "train['Dietary Habits'] = train['Dietary Habits'].fillna(train['Dietary Habits'].mode()[0])\n",
    "\n",
    "binary_cols = ['Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n",
    "for col in binary_cols:\n",
    "    train[col] = train[col].map({'Yes': 1, 'No': 0})\n",
    "train['Gender'] = train['Gender'].map({'Male': 1, 'Female': 0})\n",
    "train['Working Professional or Student'] = train['Working Professional or Student'].map({'Working Professional': 1, 'Student': 0})\n",
    "\n",
    "# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§\n",
    "scale_cols = ['Sleep Duration', 'Work/Study Hours', 'Financial Stress']\n",
    "# scale_cols = ['Age', 'Sleep Duration', 'Work/Study Hours', 'Financial Stress']\n",
    "scaler = StandardScaler()\n",
    "train[scale_cols] = scaler.fit_transform(train[scale_cols])\n",
    "\n",
    "# ì›í•«ì¸ì½”ë”© + íƒ€ê²Ÿì¸ì½”ë”©\n",
    "train = pd.get_dummies(train, columns=['degree_group', 'Dietary Habits'], drop_first=True) # ë‹¤ì¤‘ê³µì„ ì„± ë°©ì§€ë¥¼ ìœ„í•´ ì²« ë²ˆì§¸ ë²”ì£¼ëŠ” ì œê±°\n",
    "for col in ['Profession', 'City']:\n",
    "    target_mean = train.groupby(col)['Depression'].mean()\n",
    "    train[col + '_target'] = train[col].map(target_mean)\n",
    "    \n",
    "\n",
    "train.drop(columns=['Profession', 'City'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badb1b3-099f-4837-a6e9-0949ab7bd002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "09eb5125-6071-411f-8ce7-b91d199cf786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:45:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:45:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:45:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:45:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:45:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:45:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:45:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:45:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:45:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/choieunseo/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [20:45:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (10íšŒ): [0.938  0.9386 0.9383 0.9364 0.9391 0.9372 0.9384 0.9389 0.94   0.9377]\n",
      "Recall (10íšŒ):    [0.8061 0.8092 0.8114 0.8038 0.8074 0.8086 0.8065 0.8156 0.8191 0.8109]\n",
      "\n",
      "í‰ê·  Accuracy: 0.9383\n",
      "í‰ê·  Recall:   0.8099\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "accuracy_list = []\n",
    "recall_list = []\n",
    "\n",
    "# 10íšŒ ë°˜ë³µ\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        train.drop(columns=['Depression']),\n",
    "        train['Depression'],\n",
    "        test_size=0.3,\n",
    "        stratify=train['Depression'],\n",
    "        random_state=i  # ë°˜ë³µë§ˆë‹¤ seed ë³€ê²½\n",
    "    )\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "\n",
    "    accuracy_list.append(acc)\n",
    "    recall_list.append(rec)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"Accuracy (10íšŒ):\", np.round(accuracy_list, 4))\n",
    "print(\"Recall (10íšŒ):   \", np.round(recall_list, 4))\n",
    "print(f\"\\ní‰ê·  Accuracy: {np.mean(accuracy_list):.4f}\")\n",
    "print(f\"í‰ê·  Recall:   {np.mean(recall_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4989d1-aa50-4dff-a7c6-2e9b252c9617",
   "metadata": {},
   "source": [
    "## 4. ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc814e6-1f42-4392-b63b-34e04dadb332",
   "metadata": {},
   "source": [
    "| **ê°€ì„¤**                           | **Accuracy (í‰ê· )** | **Recall (í‰ê· )** | **Accuracy (10íšŒ ì¸¡ì •ê°’)**                                                            | **Recall (10íšŒ ì¸¡ì •ê°’)**                                                              |\n",
    "| -------------------------------- | ----------------- | --------------- | --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- |\n",
    "| **ê°€ì„¤ 1**<br>(ê³ ìœ ê°’ ë° ê²°ì¸¡ì¹˜ ë§ì€ ì»¬ëŸ¼ ì‚­ì œ) | 0.9130            | 0.6765          | \\[0.913, 0.9132, 0.9135, 0.9134, 0.9134, 0.9126, 0.914, 0.9118, 0.9131, 0.9118]   | \\[0.6815, 0.6733, 0.6793, 0.6747, 0.679, 0.6728, 0.6823, 0.6711, 0.6777, 0.6731]  |\n",
    "| **ê°€ì„¤ 2**<br>(í•™ìƒ/ì§ì¥ì¸ ë¶„ë¦¬ ëª¨ë¸ë§)      | 0.9390        | 0.8125     | \\[0.9387, 0.9391, 0.9398, 0.9383, 0.9374, 0.9377, 0.9394, 0.9402, 0.9386, 0.9405] | \\[0.8151, 0.8144, 0.8152, 0.8053, 0.8082, 0.8069, 0.8109, 0.8193, 0.81, 0.8197]   |\n",
    "| **ê°€ì„¤ 3**<br>(ê²°ì¸¡ì¹˜ ì„í“¨íŒ… + íŒŒìƒë³€ìˆ˜ ìƒì„±)  | 0.9383            | 0.8099          | \\[0.938, 0.9386, 0.9383, 0.9364, 0.9391, 0.9372, 0.9384, 0.9389, 0.94, 0.9377]    | \\[0.8061, 0.8092, 0.8114, 0.8038, 0.8074, 0.8086, 0.8065, 0.8156, 0.8191, 0.8109] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61a3b120-e6ab-4676-9b39-6a33a049a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "# Accuracy ë°ì´í„°\n",
    "acc_h1 = [0.913, 0.9132, 0.9135, 0.9134, 0.9134, 0.9126, 0.914, 0.9118, 0.9131, 0.9118]\n",
    "acc_h2 = [0.9387, 0.9391, 0.9398, 0.9383, 0.9374, 0.9377, 0.9394, 0.9402, 0.9386, 0.9405]\n",
    "acc_h3 = [0.938, 0.9386, 0.9383, 0.9364, 0.9391, 0.9372, 0.9384, 0.9389, 0.94, 0.9377]\n",
    "\n",
    "# Recall ë°ì´í„°\n",
    "rec_h1 = [0.6815, 0.6733, 0.6793, 0.6747, 0.679, 0.6728, 0.6823, 0.6711, 0.6777, 0.6731]\n",
    "rec_h2 = [0.8151, 0.8144, 0.8152, 0.8053, 0.8082, 0.8069, 0.8109, 0.8193, 0.81, 0.8197]\n",
    "rec_h3 = [0.8061, 0.8092, 0.8114, 0.8038, 0.8074, 0.8086, 0.8065, 0.8156, 0.8191, 0.8109]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92701354-751c-4d30-8de0-b9a17f5c4472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leveneâ€™s test for Accuracy:\n",
      "Statistic = 0.7865, p-value = 0.4656\n",
      "\n",
      "Leveneâ€™s test for Recall:\n",
      "Statistic = 0.3843, p-value = 0.6846\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import levene\n",
    "\n",
    "# Accuracyì— ëŒ€í•œ ë“±ë¶„ì‚°ì„± ê²€ì •\n",
    "levene_acc = levene(acc_h1, acc_h2, acc_h3)\n",
    "print(\"Leveneâ€™s test for Accuracy:\")\n",
    "print(f\"Statistic = {levene_acc.statistic:.4f}, p-value = {levene_acc.pvalue:.4f}\")\n",
    "\n",
    "# Recallì— ëŒ€í•œ ë“±ë¶„ì‚°ì„± ê²€ì •\n",
    "levene_rec = levene(rec_h1, rec_h2, rec_h3)\n",
    "print(\"\\nLeveneâ€™s test for Recall:\")\n",
    "print(f\"Statistic = {levene_rec.statistic:.4f}, p-value = {levene_rec.pvalue:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76c64b-fd86-4c00-9867-ba5fbaf5e466",
   "metadata": {},
   "source": [
    "Leveneâ€™s(ë¦¬ë¹ˆìŠ¤) ê²€ì •ì˜ ê·€ë¬´ê°€ì„¤(Hâ‚€):\n",
    "\n",
    "ì„¸ ê·¸ë£¹ì˜ ë¶„ì‚°ì´ ì„œë¡œ ê°™ë‹¤ (ë“±ë¶„ì‚°ì´ë‹¤)\n",
    "\n",
    "p-valueê°€ 0.05ë³´ë‹¤ í¬ë¯€ë¡œ, ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•  ìˆ˜ ì—†ìŒ\n",
    "\n",
    "ì¦‰, ì„¸ ì§‘ë‹¨ì˜ ë¶„ì‚°ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ê²Œ ë‹¤ë¥´ì§€ ì•Šë‹¤ â†’ ë“±ë¶„ì‚°ì„±ì´ ìˆë‹¤ëŠ” í•´ì„ì´ ê°€ëŠ¥í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d74b397b-406f-4540-a125-1c5d36848051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShapiroResult(statistic=np.float64(0.8991025445855032), pvalue=np.float64(0.21417490269488815))\n",
      "ShapiroResult(statistic=np.float64(0.9726550536536342), pvalue=np.float64(0.9143110414963821))\n",
      "ShapiroResult(statistic=np.float64(0.990107053988515), pvalue=np.float64(0.9969422624001061))\n",
      "ShapiroResult(statistic=np.float64(0.9207633142090637), pvalue=np.float64(0.36336527152231446))\n",
      "ShapiroResult(statistic=np.float64(0.9431801584219465), pvalue=np.float64(0.5889112848621444))\n",
      "ShapiroResult(statistic=np.float64(0.9361914536774865), pvalue=np.float64(0.5114632857063324))\n"
     ]
    }
   ],
   "source": [
    "print(stats.shapiro(acc_h1))\n",
    "print(stats.shapiro(acc_h2))\n",
    "print(stats.shapiro(acc_h3))\n",
    "\n",
    "print(stats.shapiro(rec_h1))\n",
    "print(stats.shapiro(rec_h2))\n",
    "print(stats.shapiro(rec_h3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c733e980-ad6f-4674-a0d9-8b65f399e35f",
   "metadata": {},
   "source": [
    "ğŸ” Shapiro-Wilk ì •ê·œì„± ê²€ì • ìš”ì•½\n",
    "Shapiro-Wilk ê²€ì •ì€ ì£¼ì–´ì§„ ë°ì´í„°ê°€ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ”ì§€ë¥¼ í™•ì¸í•˜ëŠ” ê²€ì •ì…ë‹ˆë‹¤.\n",
    "\n",
    "ê·€ë¬´ê°€ì„¤(Hâ‚€): ë°ì´í„°ëŠ” ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤.\n",
    "\n",
    "ëŒ€ë¦½ê°€ì„¤(Hâ‚): ë°ì´í„°ëŠ” ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤.\n",
    "\n",
    "p-value > 0.05 â†’ ì •ê·œì„± ë§Œì¡± (Hâ‚€ ì±„íƒ)\n",
    "\n",
    "p-value â‰¤ 0.05 â†’ ì •ê·œì„± ìœ„ë°˜ (Hâ‚€ ê¸°ê°)\n",
    "\n",
    "| í†µê³„ëŸ‰ (`statistic`) | p-value | ì •ê·œì„± ë§Œì¡± ì—¬ë¶€       |\n",
    "| ----------------- | ------- | --------------- |\n",
    "| 0.8991            | 0.2142  | âœ… ë§Œì¡± (p > 0.05) |\n",
    "| 0.9727            | 0.9143  | âœ… ë§Œì¡±            |\n",
    "| 0.9901            | 0.9969  | âœ… ë§Œì¡±            |\n",
    "| 0.9208            | 0.3634  | âœ… ë§Œì¡±            |\n",
    "| 0.9432            | 0.5889  | âœ… ë§Œì¡±            |\n",
    "| 0.9362            | 0.5115  | âœ… ë§Œì¡±            |\n",
    "\n",
    "ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ p-valueê°€ ëª¨ë‘ 0.05ë³´ë‹¤ í½ë‹ˆë‹¤.\n",
    "ë”°ë¼ì„œ ëª¨ë“  ì¼€ì´ìŠ¤ì—ì„œ ì •ê·œì„± ê°€ì •ì„ ë§Œì¡±í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ê²°ê³¼ëŠ” ANOVAë‚˜ Tukey HSDì²˜ëŸ¼ ì •ê·œì„± ê°€ì •ì„ ì „ì œë¡œ í•˜ëŠ” í†µê³„ê²€ì •ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì— ì í•©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "055ce74b-46c6-499e-9b93-f299731a1ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Accuracy] F = 2531.0216, p = 0.0000\n",
      "[Recall]   F = 2915.2928, p = 0.0000\n"
     ]
    }
   ],
   "source": [
    "f_acc, p_acc = stats.f_oneway(acc_h1, acc_h2, acc_h3)\n",
    "\n",
    "# ğŸ¯ Recallì— ëŒ€í•œ ë¶„ì‚°ë¶„ì„\n",
    "f_rec, p_rec = stats.f_oneway(rec_h1, rec_h2, rec_h3)\n",
    "\n",
    "print(f\"[Accuracy] F = {f_acc:.4f}, p = {p_acc:.4f}\")\n",
    "print(f\"[Recall]   F = {f_rec:.4f}, p = {p_rec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77035732-44b1-46d7-89d6-9b7694b861fb",
   "metadata": {},
   "source": [
    "âœ… 1. ì •ê·œì„± ë§Œì¡±\n",
    "ì•ì„œ Shapiro-Wilk ê²€ì • ê²°ê³¼ì—ì„œ ëª¨ë“  ê·¸ë£¹ì˜ ì •ê·œì„± ë§Œì¡± â†’ OK\n",
    "âœ”ï¸ ANOVA ìˆ˜í–‰ ê°€ëŠ¥ ì¡°ê±´ ì¶©ì¡±\n",
    "\n",
    "âœ… 2. ë¶„ì‚°ë¶„ì„ (ANOVA) ê²°ê³¼\n",
    "```\n",
    "[Accuracy] F = 2531.0216, p = 0.0000\n",
    "[Recall]   F = 2915.2928, p = 0.0000\n",
    "```\n",
    "ğŸ” í•´ì„:\n",
    "ê·€ë¬´ê°€ì„¤(Hâ‚€): ì„¸ ê·¸ë£¹ì˜ í‰ê· ì€ ëª¨ë‘ ê°™ë‹¤\n",
    "\n",
    "ëŒ€ë¦½ê°€ì„¤(Hâ‚): ì ì–´ë„ í•˜ë‚˜ì˜ ê·¸ë£¹ì€ í‰ê· ì´ ë‹¤ë¥´ë‹¤\n",
    "\n",
    "âœ… p-value â‰ª 0.05 â†’ ê·€ë¬´ê°€ì„¤ ê¸°ê°\n",
    "ğŸ‘‰ ì¦‰, ì„¸ ê·¸ë£¹ ì¤‘ ì ì–´ë„ í•˜ë‚˜ëŠ” í‰ê· ì´ ìœ ì˜í•˜ê²Œ ë‹¤ë¥´ë‹¤!\n",
    "ë”°ë¼ì„œ:\n",
    "\n",
    "âœ… 3. ì‚¬í›„ê²€ì • (Tukey HSD) ìˆ˜í–‰ í•„ìš”\n",
    "ë¶„ì‚°ë¶„ì„ì€ â€œì°¨ì´ê°€ ìˆë‹¤â€ëŠ” ê²ƒê¹Œì§€ë§Œ ì•Œë ¤ì¤ë‹ˆë‹¤.\n",
    "ì–´ëŠ ê·¸ë£¹ ê°„ì— ì°¨ì´ê°€ ìˆëŠ”ì§€ëŠ” ì•Œë ¤ì£¼ì§€ ì•Šê¸° ë•Œë¬¸ì—,\n",
    "â†’ ë°”ë¡œ ì´ì–´ì„œ Tukey HSD ì‚¬í›„ê²€ì •ì„ í•´ì•¼ í•©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fc7984e2-ef59-4958-b5f7-e94186359941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Multiple Comparison of Means - Tukey HSD, FWER=0.05      \n",
      "===============================================================\n",
      "   group1       group2    meandiff p-adj   lower  upper  reject\n",
      "---------------------------------------------------------------\n",
      "hypothesis_1 hypothesis_2    0.026    0.0   0.025  0.027   True\n",
      "hypothesis_1 hypothesis_3   0.0253    0.0  0.0242 0.0263   True\n",
      "hypothesis_2 hypothesis_3  -0.0007 0.2213 -0.0017 0.0003  False\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Accuracy ì˜ˆì‹œ\n",
    "acc_1 = [0.913, 0.9132, 0.9135, 0.9134, 0.9134, 0.9126, 0.914, 0.9118, 0.9131, 0.9118]\n",
    "acc_2 = [0.9387, 0.9391, 0.9398, 0.9383, 0.9374, 0.9377, 0.9394, 0.9402, 0.9386, 0.9405]\n",
    "acc_3 = [0.938, 0.9386, 0.9383, 0.9364, 0.9391, 0.9372, 0.9384, 0.9389, 0.94, 0.9377]\n",
    "\n",
    "# ê·¸ë£¹ ì´ë¦„ ì§€ì •\n",
    "acc = acc_1 + acc_2 + acc_3\n",
    "group = ['hypothesis_1']*10 + ['hypothesis_2']*10 + ['hypothesis_3']*10\n",
    "\n",
    "\n",
    "# DataFrame ìƒì„±\n",
    "df = pd.DataFrame({'accuracy': acc, 'group': group})\n",
    "\n",
    "# Tukeyâ€™s HSD test ìˆ˜í–‰\n",
    "tukey_result = pairwise_tukeyhsd(endog=df['accuracy'], groups=df['group'], alpha=0.05)\n",
    "print(tukey_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7d81c-0d9b-461e-bac3-917b87cbc508",
   "metadata": {},
   "source": [
    "| ê·¸ë£¹ëª…           | í‰ê·  AUC  |\n",
    "| ------------- | ------- |\n",
    "| hypothesis\\_1 | ì•½ 0.913 |\n",
    "| hypothesis\\_2 | ì•½ 0.939 |\n",
    "| hypothesis\\_3 | ì•½ 0.938 |\n",
    "\n",
    "hypothesis_1ì€ ë‹¤ë¥¸ ë‘ ê·¸ë£¹ë³´ë‹¤ AUCê°€ ìœ ì˜í•˜ê²Œ ë‚®ìŒ\n",
    "\n",
    "hypothesis_2ì™€ hypothesis_3 ê°„ì—ëŠ” ì°¨ì´ ì—†ìŒ â†’ ì„±ëŠ¥ ìœ ì‚¬\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bd8eb541-7855-4500-96cc-d6cd4ff7f697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Multiple Comparison of Means - Tukey HSD, FWER=0.05      \n",
      "===============================================================\n",
      "   group1       group2    meandiff p-adj   lower  upper  reject\n",
      "---------------------------------------------------------------\n",
      "hypothesis_1 hypothesis_2    0.136    0.0   0.131 0.1411   True\n",
      "hypothesis_1 hypothesis_3   0.1334    0.0  0.1283 0.1384   True\n",
      "hypothesis_2 hypothesis_3  -0.0026 0.4097 -0.0077 0.0024  False\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Recall ê°’ ì˜ˆì‹œ\n",
    "rec_1 = [0.6815, 0.6733, 0.6793, 0.6747, 0.6790, 0.6728, 0.6823, 0.6711, 0.6777, 0.6731]\n",
    "rec_2 = [0.8151, 0.8144, 0.8152, 0.8053, 0.8082, 0.8069, 0.8109, 0.8193, 0.81, 0.8197]\n",
    "rec_3 = [0.8061, 0.8092, 0.8114, 0.8038, 0.8074, 0.8086, 0.8065, 0.8156, 0.8191, 0.8109]\n",
    "\n",
    "# ê·¸ë£¹ ì´ë¦„ ì§€ì •\n",
    "recall = rec_1 + rec_2 + rec_3\n",
    "group = ['hypothesis_1'] * 10 + ['hypothesis_2'] * 10 + ['hypothesis_3'] * 10\n",
    "\n",
    "# DataFrame ìƒì„±\n",
    "df = pd.DataFrame({'recall': recall, 'group': group})\n",
    "\n",
    "# Tukeyâ€™s HSD test ìˆ˜í–‰\n",
    "tukey_result = pairwise_tukeyhsd(endog=df['recall'], groups=df['group'], alpha=0.05)\n",
    "\n",
    "print(tukey_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a994a62-ba3f-4eb7-af28-a68e7f6ca998",
   "metadata": {},
   "source": [
    "ğŸ” âœ… ë¹„êµ: hypothesis_2 vs hypothesis_3\n",
    "\n",
    "ğŸ“Œ ê·€ë¬´ê°€ì„¤ (Null Hypothesis, Hâ‚€)\n",
    "ë‘ ê·¸ë£¹ì˜ í‰ê· ì€ ê°™ë‹¤\n",
    "â†’ ğœ‡â‚‚ = ğœ‡â‚ƒ\n",
    "â†’ recallì˜ í‰ê·  ì°¨ì´ = 0\n",
    "\n",
    "ğŸ“Œ ëŒ€ë¦½ê°€ì„¤ (Alternative Hypothesis, Hâ‚)\n",
    "ë‘ ê·¸ë£¹ì˜ í‰ê· ì€ ë‹¤ë¥´ë‹¤\n",
    "â†’ ğœ‡â‚‚ â‰  ğœ‡â‚ƒ\n",
    "â†’ recallì˜ í‰ê·  ì°¨ì´ â‰  0\n",
    "\n",
    "ğŸ”¬ í•´ì„ ê²°ê³¼\n",
    "p-value = 0.4097 > 0.05 â†’ ê·€ë¬´ê°€ì„¤ ê¸°ê°í•˜ì§€ ëª»í•¨\n",
    "\n",
    "ì¦‰, í†µê³„ì ìœ¼ë¡œ í‰ê· ì— ì°¨ì´ê°€ ìˆë‹¤ê³  ë³´ê¸° ì–´ë µë‹¤\n",
    "\n",
    "âœ… ê²°ë¡  ì •ë¦¬\n",
    "ê°€ì„¤(Hâ‚€): â€œhypothesis_2ì™€ hypothesis_3ëŠ” ê°™ì€ í‰ê·  recallì„ ê°€ì§„ë‹¤.â€\n",
    "ê²°ê³¼: â€œë°ì´í„° ìƒì—ì„œëŠ” ì°¨ì´ê°€ ë‚˜ì§€ë§Œ, ì´ ì°¨ì´ê°€ ìš°ì—°ì— ì˜í•œ ê²ƒì¼ ìˆ˜ë„ ìˆë‹¤â€\n",
    "â†’ ë”°ë¼ì„œ Hâ‚€ë¥¼ ìœ ì§€í•¨ â†’ \"ì°¨ì´ ì—†ìŒ\"ìœ¼ë¡œ íŒë‹¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961bf146-42b5-44c5-bb49-0a9e3dac5702",
   "metadata": {},
   "source": [
    "âŒ hypothesis_2 vs hypothesis_3\n",
    "í‰ê·  ì°¨ì´: -0.0026\n",
    "\n",
    "p-value: 0.4097\n",
    "\n",
    "ì‹ ë¢°êµ¬ê°„: (-0.0077, 0.0024)\n",
    "\n",
    "reject = False â†’ ì°¨ì´ ì—†ìŒ\n",
    "\n",
    "ğŸ‘‰ ê²°ë¡ : hypothesis_2ì™€ hypothesis_3 ê°„ recall ì°¨ì´ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "hypothesis_2ì™€ hypothesis_3 ê°„ì—ëŠ” ìœ ì˜ë¯¸í•œ ì°¨ì´ ì—†ìŒ â†’ ì„±ëŠ¥ì€ ìœ ì‚¬í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d999540a-f7d7-4197-bda6-012c5f488af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Multiple Comparison of Means - Tukey HSD, FWER=0.05      \n",
      "===============================================================\n",
      "   group1       group2    meandiff p-adj   lower  upper  reject\n",
      "---------------------------------------------------------------\n",
      "hypothesis_1 hypothesis_2    0.136    0.0   0.131 0.1411   True\n",
      "hypothesis_1 hypothesis_3   0.1334    0.0  0.1283 0.1384   True\n",
      "hypothesis_2 hypothesis_3  -0.0026 0.4097 -0.0077 0.0024  False\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Recall ë°ì´í„°\n",
    "recall_1 = [0.6815, 0.6733, 0.6793, 0.6747, 0.679, 0.6728, 0.6823, 0.6711, 0.6777, 0.6731]  # ë‚®ìŒ\n",
    "recall_2 = [0.8151, 0.8144, 0.8152, 0.8053, 0.8082, 0.8069, 0.8109, 0.8193, 0.81, 0.8197]  # ë†’ìŒ\n",
    "recall_3 = [0.8061, 0.8092, 0.8114, 0.8038, 0.8074, 0.8086, 0.8065, 0.8156, 0.8191, 0.8109]  # ì¤‘ê°„\n",
    "\n",
    "# í‰ê·  ë†’ì€ ìˆœìœ¼ë¡œ group ë¼ë²¨ ë§¤í•‘\n",
    "recall = recall_1 + recall_2 + recall_3\n",
    "group = ['hypothesis_1']*10 + ['hypothesis_2']*10 + ['hypothesis_3']*10\n",
    "\n",
    "# DataFrame ìƒì„±\n",
    "df = pd.DataFrame({'recall': recall, 'group': group})\n",
    "\n",
    "# Tukey HSD ìˆ˜í–‰\n",
    "tukey_result = pairwise_tukeyhsd(endog=df['recall'], groups=df['group'], alpha=0.05)\n",
    "print(tukey_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb5517-b429-4cc6-b70a-517c6e446d08",
   "metadata": {},
   "source": [
    "hypothesis_2 vs hypothesis_3 ê°„ ì°¨ì´ëŠ” ì‘ê³  í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•Šë‹¤\n",
    "\n",
    "í‰ê·  ì°¨ì´ = -0.0026 (ë§¤ìš° ì‘ìŒ)\n",
    "\n",
    "ì‹ ë¢°êµ¬ê°„: 0ì„ í¬í•¨í•¨\n",
    "\n",
    "p-value = 0.4097 â†’ ê·€ë¬´ê°€ì„¤ ê¸°ê°í•  ìˆ˜ ì—†ìŒ â†’ ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ë™ì¼í•˜ë‹¤ê³  ë´„\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08733f-b1a4-4de1-9407-5dac327d73bb",
   "metadata": {},
   "source": [
    "ëª¨ë¸ ê°„ í‰ê·  ì„±ëŠ¥ ì°¨ì´ë¥¼ í†µê³„ì ìœ¼ë¡œ ê²€ì¦í•˜ê¸° ìœ„í•´ ANOVA ë¶„ì„ì„ ìˆ˜í–‰í•œ ë’¤,\n",
    "\n",
    "ê°œë³„ ëª¨ë¸ ìŒ ê°„ì˜ ìœ ì˜ë¯¸í•œ ì°¨ì´ ì—¬ë¶€ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ Tukey HSD ì‚¬í›„ê²€ì •ì„ ì¶”ê°€ë¡œ ì‹œí–‰í•˜ì˜€ë‹¤.\n",
    "\n",
    "íŠ¹íˆ, ê°€ì„¤ 2ì™€ ê°€ì„¤ 3ì€ í‰ê·  ì„±ëŠ¥ì´ ê·¼ì ‘í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚˜,\n",
    "\n",
    "ë‘ ëª¨ë¸ì˜ ì‹¤ì œ ì„±ëŠ¥ì´ ìœ ì‚¬í•œì§€ë¥¼ í†µê³„ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ë° ì´ ì‚¬í›„ê²€ì •ì´ ì‚¬ìš©ë˜ì—ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
